{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "> By default, ReadNext uses Hugging Face models that it downloads locally to generate the embeddings. Optionally, it can use external embedding services. At the moment, it is only integrated with the Cohere Embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import chromadb\n",
    "import cohere\n",
    "import os\n",
    "import torch\n",
    "from chromadb.errors import IDAlreadyExistsError\n",
    "from functools import cache \n",
    "from pypdf import PdfReader\n",
    "from readnext.arxiv_categories import exists\n",
    "from readnext.arxiv_sync import get_docs_path\n",
    "from rich import print\n",
    "from rich.progress import Progress\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Embedding Model\n",
    "\n",
    "To be able to use local embedding model, the first step is to download them from Hugging Face using their Transformers library and save them locally on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def download_embedding_model(model_path: str, model_name: str):\n",
    "    \"\"\"Download a Hugging Face model and tokenizer to the specified directory\"\"\"\n",
    "    # Check if the directory already exists\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    # Save the model and tokenizer to the specified directory\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_embedding_model('test-download/', 'prajjwal1/bert-tiny')\n",
    "\n",
    "assert os.path.exists('test-download/config.json')\n",
    "assert os.path.exists('test-download/pytorch_model.bin')\n",
    "assert os.path.exists('test-download/special_tokens_map.json')\n",
    "assert os.path.exists('test-download/tokenizer_config.json')\n",
    "assert os.path.exists('test-download/vocab.txt')\n",
    "\n",
    "# tears down \n",
    "rmtree('test-download/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Embedding Model\n",
    "\n",
    "Once the models are available locally, the next step is to load them in memory to be able to use them to create the embeddings for the PDF files. Because `load_embedding_model` can be called numerous time, we do memoize the result to speed up the process. There is no need to use a LRU cache here since only a single item should be cached anyway, so let's simplify the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@cache\n",
    "def load_embedding_model(model_path: str):\n",
    "    \"\"\"Load a Hugging Face model and tokenizer from the specified directory\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_embedding_model('test-download/', 'prajjwal1/bert-tiny')\n",
    "\n",
    "model, tokenizer = load_embedding_model('test-download/')\n",
    "\n",
    "assert model is not None\n",
    "assert tokenizer is not None\n",
    "\n",
    "# tears down \n",
    "rmtree('test-download/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed (Local Model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def embed_text(text: str, model, tokenizer):\n",
    "    \"\"\"Embed a text using a Hugging Face model and tokenizer\"\"\"\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        # Perform pooling. In this case, cls pooling.\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "\n",
    "    embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_embedding_model('test-download/', 'BAAI/bge-base-en')\n",
    "\n",
    "model, tokenizer = load_embedding_model('test-download/')\n",
    "\n",
    "tensor = embed_text('Hello world!', model, tokenizer)\n",
    "\n",
    "assert len(tensor.tolist()[0]) == 128\n",
    "\n",
    "# tears down \n",
    "rmtree('test-download/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embedding System\n",
    "\n",
    "We need to be able to easily identify the embedding system currently configured by the user. This is a utility function to simply the comprehension of the code elsewhere in the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def embedding_system() -> str:\n",
    "    \"\"\"Return a unique identifier for the embedding system currently in use\"\"\"\n",
    "\n",
    "    if os.environ.get('EMBEDDING_SYSTEM') == 'BAAI/bge-base-en':\n",
    "        return 'baai-bge-base-en'\n",
    "    elif os.environ.get('EMBEDDING_SYSTEM') == 'cohere':\n",
    "        return 'cohere'\n",
    "        embeddings = co.embed([text]).embeddings\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Embeddings (From any supporter system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_embeddings(text: str) -> list:\n",
    "    \"\"\"Get embeddings for a text using any supported embedding system.\"\"\"\n",
    "\n",
    "    match embedding_system():\n",
    "        case 'baai-bge-base-en':\n",
    "            model, tokenizer = load_embedding_model(os.environ.get('MODELS_PATH'))\n",
    "            return embed_text(text, model, tokenizer).tolist()\n",
    "        case 'cohere':\n",
    "            co = cohere.Client(os.environ.get('COHERE_API_KEY'))\n",
    "            return co.embed([text]).embeddings\n",
    "        case other:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to Text\n",
    "\n",
    "The library PdfReader is used to extract the text from the PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def pdf_to_text(file_path: str) -> str:\n",
    "    \"\"\"Read a PDF file and output it as a text string.\"\"\"\n",
    "    with open(file_path, 'rb') as pdf_file_obj:\n",
    "        pdf_reader = PdfReader(pdf_file_obj)\n",
    "        text = ''\n",
    "\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pdf_to_text(\"../tests/assets/test.pdf\") == \"this is a test\"\n",
    "assert pdf_to_text(\"../tests/assets/test.pdf\") != \"this is a test foo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get PDF files from a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def get_pdfs_from_folder(folder_path: str) -> list:\n",
    "    \"\"\"Given a folder path, return all the PDF files existing in that folder.\"\"\"\n",
    "    return [pdf for pdf in os.listdir(folder_path) if pdf.endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_pdfs_from_folder(\"../tests/assets/\") == ['test.pdf']\n",
    "assert get_pdfs_from_folder(\"../tests/assets/\") != ['test.pdf', 'foo.pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Chroma Collection Name\n",
    "\n",
    "It is important that the number of dimensions of the embedding is the same in a Chroma collection and when it gets queried. For example, depending what the users want to use, he may at one time use the local embedding model and at another time use the Cohere embedding service. In both cases, the number of dimensions of the embedding will be different. To avoid this problem, we use the name of the collection to determine the number of dimensions of the embedding. This way, the number of dimensions will be the same for a given collection, no matter what embedding model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chroma_collection_name(name: str) -> str:\n",
    "    \"\"\"Get the name of the ChromaDB collection to use.\"\"\"\n",
    "    \n",
    "    return os.environ.get('CHROMA_COLLECTION_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed all papers of a arXiv category\n",
    "\n",
    "The embedding database management system ReadNext uses is [Chroma](https://www.trychroma.com/).\n",
    "\n",
    "The embedding DBMS is organized as follows:\n",
    "\n",
    " - Each category (sub or top categories) become a collection of embeddings\n",
    " - We have one global collection named `all` that contains all the embeddings of every known categories\n",
    "\n",
    "When a new arXiv category is being processing, all the embeddings of the papers it contains will be added to the collection related to its category, and to the global collection.\n",
    "\n",
    "For the category collection, we have to prefix each category with `_arxiv` to avoid the restriction that Chroma won't accept a collection name with less than three characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def embed_category_papers(category: str) -> bool:\n",
    "    \"\"\"Given a ArXiv category, create the embeddings for each of the PDF paper existing locally.\n",
    "    Embeddings is currently using Cohere's embedding service.\n",
    "    Returns True if successful, False otherwise.\"\"\"\n",
    " \n",
    "    chroma_client = chromadb.PersistentClient(path=os.environ.get('CHROMA_DB_PATH'))\n",
    "\n",
    "    if exists(category):\n",
    "        # We create two Chroma collection of embeddings:\n",
    "        #   1. a general one with all and every embeddings called 'all'\n",
    "        #   2. one for the specific ArXiv category\n",
    "        papers_all_collection = chroma_client.get_or_create_collection(name=\"all_\" + embedding_system())\n",
    "        papers_category_collection = chroma_client.get_or_create_collection(name=\"arxiv_\" + category + '_' + embedding_system())\n",
    "\n",
    "        with Progress() as progress:\n",
    "            folder_path = get_docs_path(category)\n",
    "            pdfs = get_pdfs_from_folder(folder_path)\n",
    "\n",
    "            task = progress.add_task(\"[cyan]Embedding papers...\", total=len(pdfs))\n",
    "\n",
    "            for pdf in pdfs:\n",
    "                # check if the PDF file has already been embedded and indexed in Chromadb,\n",
    "                # let's not do all this processing if that is the case.\n",
    "                check_pdf = papers_all_collection.get(ids=[pdf])\n",
    "\n",
    "                if not progress.finished:\n",
    "                    progress.update(task, advance=1)\n",
    "\n",
    "                if len(check_pdf['ids']) == 0:\n",
    "                    doc = pdf_to_text(folder_path.rstrip('/') + '/' + pdf)\n",
    "\n",
    "                    try:\n",
    "                        papers_all_collection.add(\n",
    "                            embeddings=get_embeddings(doc),\n",
    "                            documents=[doc.encode(\"unicode_escape\").decode()], # necessary escape to prevent possible encoding errors when adding to Chroma\n",
    "                            metadatas=[{\"source\": pdf,\n",
    "                                        \"category\": category}],\n",
    "                            ids=[pdf]\n",
    "                        )\n",
    "                    except IDAlreadyExistsError:\n",
    "                        print(\"[yellow]ID already existing in Chroma DB, skipping...[/yellow]\")\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        papers_category_collection.add(\n",
    "                            embeddings=get_embeddings(doc),\n",
    "                            documents=[doc.encode(\"unicode_escape\").decode()], # necessary escape to prevent possible encoding errors when adding to Chroma\n",
    "                            metadatas=[{\"source\": pdf}],\n",
    "                            ids=[pdf]\n",
    "                        )\n",
    "                    except IDAlreadyExistsError:\n",
    "                        print(\"[yellow]ID already existing in Chroma DB, skipping...[/yellow]\")\n",
    "                        continue\n",
    "        return True\n",
    "    else:\n",
    "        print(\"[red]Can't persist embeddings in local vector db, ArXiv category not existing[/red]\")\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
